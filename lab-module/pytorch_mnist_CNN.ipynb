{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Mount the gdrive to Colab Notebook**"
      ],
      "metadata": {
        "id": "i88ZhLbJhHEV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pak2tqp8KXqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c766629c-9b08-4ba7-9269-d95f3f267a4d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Package**"
      ],
      "metadata": {
        "id": "_FwVLK-ChPXs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcsCTfIl-m6K"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Parameter & Download Dataset**"
      ],
      "metadata": {
        "id": "jx9wosoNhXZ_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBkjj_qD-swG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beaf9c93-4bd7-4d16-d110-634d06b217ad"
      },
      "source": [
        "# Parameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# Define the data transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image or numpy array to tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the dataset\n",
        "])\n",
        "\n",
        "# Load the data\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader objects\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Example of how to access the data\n",
        "for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    print(data.shape, target.shape)  # Example output: torch.Size([128, 1, 28, 28]) torch.Size([128])\n",
        "    break\n",
        "\n",
        "# input_shape variable\n",
        "input_shape = (1, img_rows, img_cols)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 15024273.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 436795.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3965222.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3930375.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "torch.Size([128, 1, 28, 28]) torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalize Dataset**\n"
      ],
      "metadata": {
        "id": "it4X9hafhjfa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzn_gW24-vGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57fbdae-0fd0-486e-a8e1-17b2f4e3c426"
      },
      "source": [
        "# Example of how to access the data\n",
        "for data, target in train_loader:\n",
        "    print(f'Batch data shape: {data.shape}')  # Example output: torch.Size([128, 1, 28, 28])\n",
        "    print(f'Batch target shape: {target.shape}')  # Example output: torch.Size([128])\n",
        "    break\n",
        "\n",
        "# Print dataset shapes\n",
        "print(f'x_train shape: {train_dataset.data.shape}')\n",
        "print(f'{len(train_dataset)} train samples')\n",
        "print(f'{len(test_dataset)} test samples')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch data shape: torch.Size([128, 1, 28, 28])\n",
            "Batch target shape: torch.Size([128])\n",
            "x_train shape: torch.Size([60000, 28, 28])\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create CNN Model**"
      ],
      "metadata": {
        "id": "8Uuc5K_BhqP7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl-3lvzy-feX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe665f9-99ee-4021-96e8-8a4457ec54cd"
      },
      "source": [
        "# Define the neural network architecture\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 12 * 12, 128)  # Adjust the size according to your input dimensions\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)  # No softmax here\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = ConvNet(num_classes=10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Print model summary\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout1): Dropout(p=0.25, inplace=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile Model and Train Model**"
      ],
      "metadata": {
        "id": "dzU-qbkChxaF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVphEMI--3sI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52a9525-909f-45f5-8459-0fc52c22d0ab"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 12\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Lists to store metrics\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Wrap train_loader with tqdm for progress bar\n",
        "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update running loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "    # Compute average loss and accuracy for the epoch\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct / total\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100 * correct / total\n",
        "\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "# Save the metrics\n",
        "torch.save({\n",
        "    'train_losses': train_losses,\n",
        "    'test_losses': test_losses,\n",
        "    'train_accuracies': train_accuracies,\n",
        "    'test_accuracies': test_accuracies\n",
        "}, 'model_metrics.pth')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/12: 100%|██████████| 469/469 [00:18<00:00, 25.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/12], Train Loss: 0.2172, Train Accuracy: 93.32%, Test Loss: 0.0436, Test Accuracy: 98.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/12: 100%|██████████| 469/469 [00:17<00:00, 27.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/12], Train Loss: 0.0805, Train Accuracy: 97.57%, Test Loss: 0.0347, Test Accuracy: 98.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/12: 100%|██████████| 469/469 [00:16<00:00, 27.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/12], Train Loss: 0.0641, Train Accuracy: 98.09%, Test Loss: 0.0318, Test Accuracy: 98.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/12: 100%|██████████| 469/469 [00:17<00:00, 26.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/12], Train Loss: 0.0489, Train Accuracy: 98.53%, Test Loss: 0.0302, Test Accuracy: 98.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/12: 100%|██████████| 469/469 [00:16<00:00, 27.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/12], Train Loss: 0.0447, Train Accuracy: 98.61%, Test Loss: 0.0287, Test Accuracy: 99.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/12: 100%|██████████| 469/469 [00:17<00:00, 26.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/12], Train Loss: 0.0391, Train Accuracy: 98.76%, Test Loss: 0.0325, Test Accuracy: 99.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/12: 100%|██████████| 469/469 [00:16<00:00, 28.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/12], Train Loss: 0.0346, Train Accuracy: 98.92%, Test Loss: 0.0292, Test Accuracy: 99.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/12: 100%|██████████| 469/469 [00:17<00:00, 27.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/12], Train Loss: 0.0318, Train Accuracy: 98.98%, Test Loss: 0.0283, Test Accuracy: 99.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/12: 100%|██████████| 469/469 [00:16<00:00, 27.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/12], Train Loss: 0.0268, Train Accuracy: 99.15%, Test Loss: 0.0301, Test Accuracy: 99.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/12: 100%|██████████| 469/469 [00:17<00:00, 27.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/12], Train Loss: 0.0253, Train Accuracy: 99.18%, Test Loss: 0.0300, Test Accuracy: 99.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/12: 100%|██████████| 469/469 [00:16<00:00, 27.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/12], Train Loss: 0.0247, Train Accuracy: 99.20%, Test Loss: 0.0306, Test Accuracy: 99.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/12: 100%|██████████| 469/469 [00:16<00:00, 27.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/12], Train Loss: 0.0234, Train Accuracy: 99.25%, Test Loss: 0.0274, Test Accuracy: 99.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot Test Dataset**"
      ],
      "metadata": {
        "id": "sh5nVAMch7df"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lnjcul6DPbH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "977b2fb5-f651-4fb7-83ff-bbdae8f12b7f"
      },
      "source": [
        "#print(x_test[500].shape)\n",
        "\n",
        "sample_index = 500\n",
        "batch_size = test_loader.batch_size\n",
        "\n",
        "batch_index = sample_index // batch_size\n",
        "sample_index_in_batch = sample_index % batch_size\n",
        "\n",
        "for i, (data, target) in enumerate(test_loader):\n",
        "    if i == batch_index:\n",
        "        # Get the data tensor for the sample at index 500\n",
        "        sample_data = data[sample_index_in_batch].squeeze().cpu().numpy()\n",
        "        break\n",
        "else:\n",
        "    print(f\"Sample {sample_index} does not exist in the test dataset.\")\n",
        "\n",
        "\n",
        "# Show the test image\n",
        "# Plot the data tensor for the sample at index 500\n",
        "plt.imshow(sample_data, cmap='gray')\n",
        "plt.title(f'Sample {sample_index}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict, Save and Load Model**\n",
        "# Answer Question 1 - Prediction"
      ],
      "metadata": {
        "id": "7m3UDpFyiEvz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMnhM9EzDJQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d363a9ff-4709-4ade-8e54-7db578922a72"
      },
      "source": [
        "#data = x_test[500]\n",
        "#dt = np.reshape(data, [1, 28, 28, 1])\n",
        "\n",
        "# Predict dt from the trained model\n",
        "\n",
        "\n",
        "# Load the image and preprocess it\n",
        "sample_data_tensor = torch.tensor(sample_data, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    ##Answer Here##\n",
        "    predicted_probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "# Convert the predicted probabilities tensor to a numpy array\n",
        "predicted_probs_np = predicted_probs.cpu().numpy()\n",
        "predicted_probs_np = np.round(predicted_probs_np, 3)\n",
        "\n",
        "\n",
        "# Get the predicted class (index with the highest probability)\n",
        "predicted_class = np.argmax(predicted_probs_np)\n",
        "print(\"Predicted class probabilities:\", predicted_probs_np)\n",
        "print(\"Predicted class:\", predicted_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class probabilities: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "Predicted class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer Question 2 - Save Model\n"
      ],
      "metadata": {
        "id": "CFCmg2SHHcX1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_rDg0pNCfx6"
      },
      "source": [
        "# Save the model's state dictionary to your Google Drive folder\n",
        "\n",
        "# Specify the file path\n",
        "model_path = '/content/drive/My Drive/Colab Notebooks/MNIST_MODEL/modelcnn.pt'\n",
        "\n",
        "# Save the state dictionary\n",
        "torch.save(model.state_dict(), model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List files in the directory\n",
        "directory_path = '/content/drive/My Drive/Colab Notebooks/MNIST_MODEL'\n",
        "files = os.listdir(directory_path)\n",
        "print(\"Files in directory:\", files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAI7yNv1_qQ2",
        "outputId": "40153caf-2e24-4d4f-c2b3-27772c016c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in directory: ['modelcnn.pt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Answer Question 3 - Load Model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dTTQlU4oId9G"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vSbn6w-Jlqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb47494-4913-4cf0-c4da-2c1253cec82d"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Load the model from your Google Drive folder\n",
        "model_path = '/content/drive/My Drive/Colab Notebooks/MNIST_MODEL/modelcnn.pt'\n",
        "\n",
        "# Initialize your model instance\n",
        "loaded_model = ConvNet()\n",
        "\n",
        "# Load the state dictionary from the file\n",
        "state_dict = torch.load(model_path)\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "loaded_model.load_state_dict(state_dict)\n",
        "\n",
        "# Determine the device to use (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move the model to the selected device\n",
        "loaded_model = loaded_model.to(device)\n",
        "\n",
        "# Move the input tensor to the selected device\n",
        "sample_data_tensor = sample_data_tensor.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs1 = loaded_model(sample_data_tensor)\n",
        "    predicted_probs1 = torch.softmax(outputs1, dim=1)\n",
        "\n",
        "# Convert the predicted probabilities tensor to a numpy array\n",
        "predicted_probs_np1 = predicted_probs1.cpu().numpy()\n",
        "predicted_probs_np1 = np.round(predicted_probs_np1, 3)\n",
        "\n",
        "# Get the predicted class (index with the highest probability)\n",
        "predicted_class1 = np.argmax(predicted_probs_np1)\n",
        "print(\"Predicted class probabilities:\", predicted_probs_np1)\n",
        "print(\"Predicted class:\", predicted_class1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class probabilities: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "Predicted class: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRxifBjPAKPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}